{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Both the raw predictions and the ground truth are lists of true and false values.\n",
    "Here, we compare the predictions and ground truth to derive confusion matrices and scoring metrics.\n",
    "## Preparation\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data\n",
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, fbeta_score, accuracy_score, roc_auc_score\n",
    "from imblearn.metrics import specificity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Data\n",
    "Begin by loading the data.\n",
    "The predictions by the supervised machine learning models and the llama models are within different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../../data/datasets/04_preprocessed'\n",
    "path_predictions = '../../../data/predictions'\n",
    "\n",
    "datasets = data.dict_from_directory(path_data, type='polars')\n",
    "\n",
    "# predictions by the supervised machine learning models\n",
    "preds_sml = data.dict_from_directory(path_predictions + '/supervised_machine_learning', type='polars')\n",
    "\n",
    "# predictions by the llama model\n",
    "preds_llama = data.dict_from_directory(path_predictions + '/llama_3_1_8B', type='polars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The llama predictions initially do not contain the true labels.\n",
    "Therefore concatenate the true lables and the index number from the dataset.\n",
    "\n",
    "Further, rename the llama prediction to better distinguish it from the sml predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_llama = {\n",
    "    subject: predictions.rename(\n",
    "        {\n",
    "            'include': 'llama',\n",
    "            'reason': 'llama_reason'\n",
    "        }\n",
    "    ).with_columns(\n",
    "        datasets[subject].select(\n",
    "            pl.col('index').alias('index'),\n",
    "            pl.col('include').alias('true_llama')\n",
    "        )\n",
    "    )\n",
    "    for subject, predictions in preds_llama.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised machine learning models only predicted the classes of articles within the test dataset.\n",
    "\n",
    "Concatenate the predictions of all models by the indices of the articles within the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_test_set = {\n",
    "    subject: predictions.join(\n",
    "        preds_llama[subject], on='index'\n",
    "    ).drop(['true_llama'])\n",
    "    for subject, predictions in preds_sml.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```joint_test_set``` variable now contains the articles within the sets sets.\n",
    "For each article there is the ground truth ``true``, the prediction by each respective model as well as llama's reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>true</th><th>logistic_regression</th><th>random_forest</th><th>support_vector_machine</th><th>naive_bayes</th><th>llama</th><th>llama_reason</th><th>title</th><th>doi</th><th>pubmed_id</th></tr><tr><td>i64</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>473</td><td>false</td><td>true</td><td>true</td><td>true</td><td>true</td><td>true</td><td>&quot; The study meets all the inclu…</td><td>&quot;The Effect of Magnesium Supple…</td><td>&quot;https://doi.org/10.2337/diacar…</td><td>9589224</td></tr><tr><td>238</td><td>false</td><td>false</td><td>false</td><td>false</td><td>true</td><td>true</td><td>&quot; The study meets all the inclu…</td><td>&quot;A Comparison of the Effects of…</td><td>&quot;https://doi.org/10.2337/diacar…</td><td>12401757</td></tr><tr><td>84</td><td>false</td><td>false</td><td>false</td><td>false</td><td>true</td><td>true</td><td>&quot; The study meets all the inclu…</td><td>&quot;Less nocturnal hypoglycemia an…</td><td>&quot;https://doi.org/10.2337/diacar…</td><td>10937510</td></tr><tr><td>26</td><td>true</td><td>true</td><td>true</td><td>true</td><td>true</td><td>true</td><td>&quot; The study meets all the inclu…</td><td>&quot;A comparison of preconstituted…</td><td>&quot;https://doi.org/10.1007/s00592…</td><td>10436254</td></tr><tr><td>390</td><td>false</td><td>false</td><td>true</td><td>false</td><td>true</td><td>true</td><td>&quot; The study meets all the inclu…</td><td>&quot;Circulating catecholamines and…</td><td>&quot;https://doi.org/10.2337/diacar…</td><td>8742566</td></tr><tr><td>99</td><td>false</td><td>false</td><td>false</td><td>false</td><td>true</td><td>true</td><td>&quot; The study meets the inclusion…</td><td>&quot;The effect of oral hypoglycaem…</td><td>&quot;nan&quot;</td><td>11070748</td></tr><tr><td>2</td><td>false</td><td>true</td><td>true</td><td>true</td><td>true</td><td>true</td><td>&quot; The study meets all the inclu…</td><td>&quot;Shortterm intensive insulin th…</td><td>&quot;nan&quot;</td><td>10085698</td></tr><tr><td>215</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>&quot; The article does not contain …</td><td>&quot;Desensitization of insulin sec…</td><td>&quot;https://doi.org/10.1016/s0006-…</td><td>12093468</td></tr><tr><td>283</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>&quot; The abstract does not mention…</td><td>&quot;NN Novo Nordisk&quot;</td><td>&quot;nan&quot;</td><td>12789615</td></tr><tr><td>108</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>&quot; The study does not meet the i…</td><td>&quot;Diastolic Dysfunction in Normo…</td><td>&quot;https://doi.org/10.2337/diacar…</td><td>11194240</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 11)\n",
       "┌───────┬───────┬─────────────┬─────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ index ┆ true  ┆ logistic_re ┆ random_fore ┆ … ┆ llama_reas ┆ title      ┆ doi        ┆ pubmed_id │\n",
       "│ ---   ┆ ---   ┆ gression    ┆ st          ┆   ┆ on         ┆ ---        ┆ ---        ┆ ---       │\n",
       "│ i64   ┆ bool  ┆ ---         ┆ ---         ┆   ┆ ---        ┆ str        ┆ str        ┆ i64       │\n",
       "│       ┆       ┆ bool        ┆ bool        ┆   ┆ str        ┆            ┆            ┆           │\n",
       "╞═══════╪═══════╪═════════════╪═════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ 473   ┆ false ┆ true        ┆ true        ┆ … ┆ The study  ┆ The Effect ┆ https://do ┆ 9589224   │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets all  ┆ of         ┆ i.org/10.2 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ the inclu… ┆ Magnesium  ┆ 337/diacar ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ Supple…    ┆ …          ┆           │\n",
       "│ 238   ┆ false ┆ false       ┆ false       ┆ … ┆ The study  ┆ A          ┆ https://do ┆ 12401757  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets all  ┆ Comparison ┆ i.org/10.2 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ the inclu… ┆ of the     ┆ 337/diacar ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ Effects    ┆ …          ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ of…        ┆            ┆           │\n",
       "│ 84    ┆ false ┆ false       ┆ false       ┆ … ┆ The study  ┆ Less       ┆ https://do ┆ 10937510  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets all  ┆ nocturnal  ┆ i.org/10.2 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ the inclu… ┆ hypoglycem ┆ 337/diacar ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ ia an…     ┆ …          ┆           │\n",
       "│ 26    ┆ true  ┆ true        ┆ true        ┆ … ┆ The study  ┆ A          ┆ https://do ┆ 10436254  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets all  ┆ comparison ┆ i.org/10.1 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ the inclu… ┆ of precons ┆ 007/s00592 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ tituted…   ┆ …          ┆           │\n",
       "│ 390   ┆ false ┆ false       ┆ true        ┆ … ┆ The study  ┆ Circulatin ┆ https://do ┆ 8742566   │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets all  ┆ g catechol ┆ i.org/10.2 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ the inclu… ┆ amines     ┆ 337/diacar ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ and…       ┆ …          ┆           │\n",
       "│ 99    ┆ false ┆ false       ┆ false       ┆ … ┆ The study  ┆ The effect ┆ nan        ┆ 11070748  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets the  ┆ of oral    ┆            ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ inclusion… ┆ hypoglycae ┆            ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ m…         ┆            ┆           │\n",
       "│ 2     ┆ false ┆ true        ┆ true        ┆ … ┆ The study  ┆ Shortterm  ┆ nan        ┆ 10085698  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meets all  ┆ intensive  ┆            ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ the inclu… ┆ insulin    ┆            ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆            ┆ th…        ┆            ┆           │\n",
       "│ 215   ┆ false ┆ false       ┆ false       ┆ … ┆ The        ┆ Desensitiz ┆ https://do ┆ 12093468  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ article    ┆ ation of   ┆ i.org/10.1 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ does not   ┆ insulin    ┆ 016/s0006- ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ contain …  ┆ sec…       ┆ …          ┆           │\n",
       "│ 283   ┆ false ┆ false       ┆ false       ┆ … ┆ The        ┆ NN Novo    ┆ nan        ┆ 12789615  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ abstract   ┆ Nordisk    ┆            ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ does not   ┆            ┆            ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ mention…   ┆            ┆            ┆           │\n",
       "│ 108   ┆ false ┆ false       ┆ false       ┆ … ┆ The study  ┆ Diastolic  ┆ https://do ┆ 11194240  │\n",
       "│       ┆       ┆             ┆             ┆   ┆ does not   ┆ Dysfunctio ┆ i.org/10.2 ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ meet the   ┆ n in       ┆ 337/diacar ┆           │\n",
       "│       ┆       ┆             ┆             ┆   ┆ i…         ┆ Normo…     ┆ …          ┆           │\n",
       "└───────┴───────┴─────────────┴─────────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_test_set['oral_hypoglycemics'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrices(y_true, y_pred) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    matrix = confusion_matrix(\n",
    "           y_true=y_true, \n",
    "           y_pred=y_pred,\n",
    "           normalize=None\n",
    "        )\n",
    "       \n",
    "    matrix_norm = confusion_matrix(\n",
    "        y_true=y_true, \n",
    "        y_pred=y_pred,\n",
    "        normalize='true'\n",
    "    )\n",
    "\n",
    "    return matrix, matrix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def generate_bootstrap_samples(\n",
    "        y_true, # true labels\n",
    "        y_pred, # predicted labels\n",
    "        n_resamples=1000, # number of bootstrap samples, 1000 is common\n",
    "    ):\n",
    "\n",
    "    # save the bootstrapped samples here\n",
    "    samples = []\n",
    "\n",
    "    # create n_resamples bootstrap samples\n",
    "    for _ in range(n_resamples):\n",
    "        indices = np.random.choice(len(y_true), len(y_true), replace=True)\n",
    "        y_true_resampled = y_true[indices]\n",
    "        y_pred_resampled = y_pred[indices]\n",
    "\n",
    "        #samples.append((y_true_resampled, y_pred_resampled))\n",
    "        samples.append(\n",
    "            {\n",
    "                'y_true': y_true_resampled,\n",
    "                'y_pred': y_pred_resampled\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Remove Macro averaging later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_with_ci(\n",
    "        bootstraps, # list of bootstrapped samples\n",
    "        metric, # tuple: name and function\n",
    "        round_ndigits=2, # number of digits to round the results\n",
    "        confidence_level=0.95 # confidence level for the CI\n",
    "):\n",
    "    \n",
    "    metric_name, metric_function = metric[0], metric[1]\n",
    "\n",
    "    # save the scores for each bootstrap sample\n",
    "    scores = []\n",
    "\n",
    "    # calculate the score for each bootstrap sample\n",
    "    for index, sample in enumerate(bootstraps):\n",
    "\n",
    "        y_true = sample['y_true']\n",
    "        y_pred = sample['y_pred']\n",
    "\n",
    "        try:\n",
    "            # calculate the score\n",
    "            if metric_name == 'f2': # pass beta argument for f2-score\n",
    "                score = metric_function(y_true, y_pred, beta=2)\n",
    "            else:\n",
    "                score = metric_function(y_true, y_pred)\n",
    "            scores.append(score)\n",
    "        except:\n",
    "            score = None\n",
    "        \n",
    "    # add the score to the distribution\n",
    "    scores = [score for score in scores if score is not None]\n",
    "    scores = np.array(scores) # transform to numpy for calculations\n",
    "    \n",
    "    # calculate the mean and lower and upper bounds\n",
    "    mean = np.mean(scores)\n",
    "    lower = np.percentile(scores, (1 - confidence_level) / 2 * 100)\n",
    "    upper = np.percentile(scores, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    # round the results\n",
    "    mean = round(mean, round_ndigits)\n",
    "    lower = round(lower, round_ndigits)\n",
    "    upper = round(upper, round_ndigits)\n",
    "\n",
    "    return mean, lower, upper, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Structures\n",
    "### Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    'logistic_regression',\n",
    "    'random_forest',\n",
    "    'support_vector_machine',\n",
    "    'naive_bayes',\n",
    "    'llama'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'precision': precision_score,\n",
    "    'recall': recall_score,\n",
    "    'f1': f1_score,\n",
    "    'f2': fbeta_score,\n",
    "    'specificity': specificity_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores Table\n",
    "The structure for the large table that will save scores and confidence intervals across all datasets and estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_context = {\n",
    "    'dataset': pl.String,\n",
    "    'estimator': pl.String,\n",
    "}\n",
    "\n",
    "schema_metrics = {\n",
    "    metric: pl.Struct(\n",
    "        [\n",
    "            pl.Field(f'{metric}_mean', pl.Float64),\n",
    "            pl.Field(f'{metric}_lower', pl.Float64),\n",
    "            pl.Field(f'{metric}_upper', pl.Float64),\n",
    "        ]\n",
    "    )\n",
    "    for metric in metrics.keys()\n",
    "}\n",
    "\n",
    "schema_scores = schema_context | schema_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Dictionary\n",
    "Every piece of the results will be saved within this dictionary for easy access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'scores': pl.DataFrame([], schema=schema_scores),\n",
    "    'datasets': {\n",
    "        subject: {\n",
    "            estimator: {\n",
    "                'classification_report': None,\n",
    "                'matrix': {},\n",
    "                'bootstrap': {\n",
    "                    'samples': None,\n",
    "                    'scores': {\n",
    "                        metric: None\n",
    "                        for metric in metrics\n",
    "                    }\n",
    "\n",
    "                }\n",
    "            }\n",
    "            for estimator in estimators\n",
    "        }\n",
    "        for subject in joint_test_set.keys()\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scores', 'adhd', 'animal_depression', 'atypical_antipsychotics', 'calcium_channel_blockers', 'oral_hypoglycemics', 'pancreatic_surgery'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_llama = {\n",
    "    'scores': pl.DataFrame([], schema=schema_scores),\n",
    "}\n",
    "\n",
    "for subject in preds_llama.keys():\n",
    "    results_llama[subject] = {\n",
    "        'classification_report': None,\n",
    "        'matrix': {},\n",
    "        'bootstrap': {\n",
    "            'samples': None,\n",
    "            'scores': {\n",
    "                metric: None\n",
    "                for metric in metrics\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "results_llama.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Results (TODO: Rework for both testset & LLM)\n",
    "Calculate the result metrics and save all metrics and accompaning data, such as classification results, confusion matrices, and bootstrap samples, within one results-dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama on whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21ac11b84b541fc9382da44f5db0789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5f7f2ac56a440b9a23c15835383477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12530464f454636965770d62fb05d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b1be4cd050488684d014bd709153e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e59edb8ca64666a898b86f9edb5ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595d3f9b2be1401db32270b949f049a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a820e4e1f54bff954d1d3e3f2ce2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subject, dataset in tqdm(\n",
    "    iterable=preds_llama.items(),\n",
    "    desc='Datasets',\n",
    "    total=len(preds_llama),\n",
    "    leave=True,\n",
    "):\n",
    "    model = 'llama'\n",
    "\n",
    "    y_true = dataset['true_llama'].to_numpy()\n",
    "    estimator = dataset['llama'].to_numpy()\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(y_true, estimator)\n",
    "    results_llama[subject]['classification_report'] = report\n",
    "\n",
    "\n",
    "    matrix, matrix_norm = confusion_matrices(y_true, estimator)\n",
    "    results_llama[subject]['matrix']['absolute'] = matrix\n",
    "    results_llama[subject]['matrix']['norm'] = matrix_norm\n",
    "\n",
    "    estimator_scores = {}\n",
    "\n",
    "    bootstraps = generate_bootstrap_samples(y_true, estimator)\n",
    "    results_llama[subject]['bootstrap']['samples'] = bootstraps\n",
    "\n",
    "    for metric, function in tqdm(\n",
    "        iterable=metrics.items(),\n",
    "        desc='Metrics',\n",
    "        total=len(metrics),\n",
    "        leave=False\n",
    "    ):\n",
    "\n",
    "        mean, lower, upper, bootstrap_scores = metric_with_ci(bootstraps, (metric, function))\n",
    "        results_llama[subject]['bootstrap']['scores'][metric] = bootstrap_scores\n",
    "\n",
    "        struct_series = pl.Series(\n",
    "                name=metric,\n",
    "                values=[(mean, lower, upper)],\n",
    "                dtype=pl.Struct(\n",
    "                    [\n",
    "                        pl.Field(f'{metric}_mean', pl.Float64),\n",
    "                        pl.Field(f'{metric}_lower', pl.Float64),\n",
    "                        pl.Field(f'{metric}_upper', pl.Float64),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        estimator_scores[metric] = struct_series[0]\n",
    "    \n",
    "    scores = pl.DataFrame(\n",
    "        {\n",
    "            est: [vals]\n",
    "            for est, vals in estimator_scores.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    scores = scores.with_columns(\n",
    "        pl.lit(subject, pl.String).alias('dataset'),\n",
    "        pl.lit(model, pl.String).alias('estimator')\n",
    "    )\n",
    "\n",
    "    column_order = ['dataset', 'estimator'] + list(metrics.keys())\n",
    "    scores = scores.select(column_order)\n",
    "\n",
    "    results_llama['scores'] = results_llama['scores'].vstack(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>dataset</th><th>estimator</th><th>accuracy</th><th>precision</th><th>recall</th><th>f1</th><th>f2</th><th>specificity</th></tr><tr><td>str</td><td>str</td><td>struct[3]</td><td>struct[3]</td><td>struct[3]</td><td>struct[3]</td><td>struct[3]</td><td>struct[3]</td></tr></thead><tbody><tr><td>&quot;adhd&quot;</td><td>&quot;llama&quot;</td><td>{0.82,0.8,0.85}</td><td>{0.12,0.07,0.17}</td><td>{1.0,1.0,1.0}</td><td>{0.21,0.13,0.3}</td><td>{0.4,0.27,0.51}</td><td>{0.82,0.79,0.85}</td></tr><tr><td>&quot;animal_depression&quot;</td><td>&quot;llama&quot;</td><td>{0.85,0.83,0.87}</td><td>{0.49,0.45,0.54}</td><td>{0.96,0.93,0.98}</td><td>{0.65,0.61,0.69}</td><td>{0.8,0.77,0.83}</td><td>{0.83,0.81,0.85}</td></tr><tr><td>&quot;atypical_antipsychotics&quot;</td><td>&quot;llama&quot;</td><td>{0.51,0.48,0.54}</td><td>{0.18,0.15,0.22}</td><td>{0.79,0.72,0.85}</td><td>{0.3,0.25,0.34}</td><td>{0.48,0.42,0.53}</td><td>{0.47,0.43,0.5}</td></tr><tr><td>&quot;calcium_channel_blockers&quot;</td><td>&quot;llama&quot;</td><td>{0.42,0.39,0.45}</td><td>{0.11,0.08,0.13}</td><td>{0.81,0.72,0.88}</td><td>{0.19,0.15,0.22}</td><td>{0.35,0.29,0.4}</td><td>{0.38,0.36,0.41}</td></tr><tr><td>&quot;oral_hypoglycemics&quot;</td><td>&quot;llama&quot;</td><td>{0.45,0.4,0.49}</td><td>{0.31,0.26,0.36}</td><td>{0.89,0.84,0.94}</td><td>{0.46,0.4,0.52}</td><td>{0.65,0.59,0.7}</td><td>{0.29,0.24,0.34}</td></tr><tr><td>&quot;pancreatic_surgery&quot;</td><td>&quot;llama&quot;</td><td>{0.62,0.61,0.62}</td><td>{0.11,0.1,0.11}</td><td>{0.8,0.78,0.82}</td><td>{0.19,0.18,0.2}</td><td>{0.35,0.34,0.36}</td><td>{0.61,0.6,0.61}</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 8)\n",
       "┌────────────┬───────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ dataset    ┆ estimator ┆ accuracy   ┆ precision  ┆ recall    ┆ f1        ┆ f2        ┆ specifici │\n",
       "│ ---        ┆ ---       ┆ ---        ┆ ---        ┆ ---       ┆ ---       ┆ ---       ┆ ty        │\n",
       "│ str        ┆ str       ┆ struct[3]  ┆ struct[3]  ┆ struct[3] ┆ struct[3] ┆ struct[3] ┆ ---       │\n",
       "│            ┆           ┆            ┆            ┆           ┆           ┆           ┆ struct[3] │\n",
       "╞════════════╪═══════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ adhd       ┆ llama     ┆ {0.82,0.8, ┆ {0.12,0.07 ┆ {1.0,1.0, ┆ {0.21,0.1 ┆ {0.4,0.27 ┆ {0.82,0.7 │\n",
       "│            ┆           ┆ 0.85}      ┆ ,0.17}     ┆ 1.0}      ┆ 3,0.3}    ┆ ,0.51}    ┆ 9,0.85}   │\n",
       "│ animal_dep ┆ llama     ┆ {0.85,0.83 ┆ {0.49,0.45 ┆ {0.96,0.9 ┆ {0.65,0.6 ┆ {0.8,0.77 ┆ {0.83,0.8 │\n",
       "│ ression    ┆           ┆ ,0.87}     ┆ ,0.54}     ┆ 3,0.98}   ┆ 1,0.69}   ┆ ,0.83}    ┆ 1,0.85}   │\n",
       "│ atypical_a ┆ llama     ┆ {0.51,0.48 ┆ {0.18,0.15 ┆ {0.79,0.7 ┆ {0.3,0.25 ┆ {0.48,0.4 ┆ {0.47,0.4 │\n",
       "│ ntipsychot ┆           ┆ ,0.54}     ┆ ,0.22}     ┆ 2,0.85}   ┆ ,0.34}    ┆ 2,0.53}   ┆ 3,0.5}    │\n",
       "│ ics        ┆           ┆            ┆            ┆           ┆           ┆           ┆           │\n",
       "│ calcium_ch ┆ llama     ┆ {0.42,0.39 ┆ {0.11,0.08 ┆ {0.81,0.7 ┆ {0.19,0.1 ┆ {0.35,0.2 ┆ {0.38,0.3 │\n",
       "│ annel_bloc ┆           ┆ ,0.45}     ┆ ,0.13}     ┆ 2,0.88}   ┆ 5,0.22}   ┆ 9,0.4}    ┆ 6,0.41}   │\n",
       "│ kers       ┆           ┆            ┆            ┆           ┆           ┆           ┆           │\n",
       "│ oral_hypog ┆ llama     ┆ {0.45,0.4, ┆ {0.31,0.26 ┆ {0.89,0.8 ┆ {0.46,0.4 ┆ {0.65,0.5 ┆ {0.29,0.2 │\n",
       "│ lycemics   ┆           ┆ 0.49}      ┆ ,0.36}     ┆ 4,0.94}   ┆ ,0.52}    ┆ 9,0.7}    ┆ 4,0.34}   │\n",
       "│ pancreatic ┆ llama     ┆ {0.62,0.61 ┆ {0.11,0.1, ┆ {0.8,0.78 ┆ {0.19,0.1 ┆ {0.35,0.3 ┆ {0.61,0.6 │\n",
       "│ _surgery   ┆           ┆ ,0.62}     ┆ 0.11}      ┆ ,0.82}    ┆ 8,0.2}    ┆ 4,0.36}   ┆ ,0.61}    │\n",
       "└────────────┴───────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_llama['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models on test-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e2870a959c42f4ad1e2cc5e32ab407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19b80eba94b488884e59bef4b1009e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\root\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf836a011c3415cac2b83a49f0e7b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009174421e6d4a0fbb320189bb109a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0498ccd0eae145d4936017e8d300a570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a963f3f69544f583259bb403f357e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88eefb464e72492786e110381bea09d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate over the datasets\n",
    "for subject, dataset in tqdm(\n",
    "    iterable=joint_test_set.items(),  # iterate over the datasets\n",
    "    desc='Datasets',\n",
    "    total=len(joint_test_set),\n",
    "    leave=True\n",
    "):\n",
    "\n",
    "\n",
    "    # save the predictions within a variable\n",
    "    predictions_only = dataset.select(\n",
    "        pl.exclude(\n",
    "            ['index', 'true', 'llama_reason', 'title', 'doi', 'pubmed_id']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    # iterate over the estimators\n",
    "    for estimator in tqdm(\n",
    "        iterable=predictions_only.iter_columns(),\n",
    "        desc='Estimators',\n",
    "        total=len(predictions_only.columns),\n",
    "        leave=False\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = estimator.name  # to access the results dictionary\n",
    "        \n",
    "        y_true = dataset['true'].to_numpy() # true labels\n",
    "\n",
    "        # predictions by the current estimator to calculate on\n",
    "        estimator = estimator.to_numpy()  \n",
    "\n",
    "        # add the classification report\n",
    "        report = classification_report(y_true, estimator)\n",
    "        results['datasets'][subject][model]['classification_report'] = report\n",
    "\n",
    "        # add the confusion matrices\n",
    "        matrix, matrix_norm = confusion_matrices(y_true, estimator)\n",
    "        results['datasets'][subject][model]['matrix']['absolute'] = matrix\n",
    "        results['datasets'][subject][model]['matrix']['norm'] = matrix_norm\n",
    "\n",
    "        # save the scores for each estimator in here\n",
    "        estimator_scores = {}\n",
    "\n",
    "        # calculate all metrics on the same set of bootstrap samples\n",
    "        bootstraps = generate_bootstrap_samples(y_true, estimator)\n",
    "        \n",
    "        # add the bootstraps to the results for traceability\n",
    "        results['datasets'][subject][model]['bootstrap']['samples'] = bootstraps\n",
    "\n",
    "        # calculate each metric with confidence intervals by bootstrapping\n",
    "        for metric, function in metrics.items():\n",
    "\n",
    "            # calculate the metric with confidence intervals\n",
    "            mean, lower, upper, bootstrap_scores = metric_with_ci(bootstraps, (metric, function))\n",
    "            results['datasets'][subject][model]['bootstrap']['scores'][metric] = bootstrap_scores\n",
    "\n",
    "            # Create a Series with the struct values\n",
    "            struct_series = pl.Series(\n",
    "                name=metric,\n",
    "                values=[(mean, lower, upper)],\n",
    "                dtype=pl.Struct(\n",
    "                    [\n",
    "                        pl.Field(f'{metric}_mean', pl.Float64),\n",
    "                        pl.Field(f'{metric}_lower', pl.Float64),\n",
    "                        pl.Field(f'{metric}_upper', pl.Float64),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # add the results struct to the list of scores\n",
    "            estimator_scores[metric] = struct_series[0]\n",
    "\n",
    "        # add the results to the scores dataframe\n",
    "        scores = pl.DataFrame({\n",
    "            estimator: [values]\n",
    "            for estimator, values in estimator_scores.items()\n",
    "        })\n",
    "        \n",
    "        scores = scores.with_columns(\n",
    "            pl.lit(subject, pl.String).alias('dataset'),\n",
    "            pl.lit(model, pl.String).alias('estimator')\n",
    "        )\n",
    "\n",
    "        # reorder the dataframe for stacking\n",
    "        column_order = ['dataset', 'estimator'] + list(metrics.keys())\n",
    "        scores = scores.select(column_order)\n",
    "\n",
    "        results['scores'] = results['scores'].vstack(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results', 'wb') as file:\n",
    "    import pickle\n",
    "    pickle.dump(results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
