{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Preprocessing\n",
    "While some preprocessing steps are unique to supervised machine learning models, others apply to all.\n",
    "\n",
    "Such general preprocessing happens in two steps.\n",
    "\n",
    "The first step is to remove whole articles that do not contribute to the classification task by:\n",
    "\n",
    "- Removing articles which appear more than once in one dataset (duplicates)\n",
    "- Removing non-English abstracts\n",
    "- Removing long abstracts which contain more than 489 words\n",
    "- Removing articles without abstracts\n",
    "\n",
    "The second step is to clean the texts of the remaining articles from noise, namely:\n",
    "\n",
    "- Removing HTML\n",
    "- Removing non-character symbols, such as digits and special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Preparation\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data # helper functions\n",
    "import polars as pl # dataframes\n",
    "import re # regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the data is stored\n",
    "directory = '../../../../data/datasets/03_pubmed'\n",
    "\n",
    "# Output directory\n",
    "output_directory = '../../../../data/datasets/04_preprocessed'\n",
    "\n",
    "# Load the data\n",
    "datasets = data.dict_from_directory(directory, type='polars', with_index='index')\n",
    "\n",
    "# individual datasets\n",
    "adhd = datasets['adhd']\n",
    "animal_depression = datasets['animal_depression']\n",
    "atypical_antipsychotics = datasets['atypical_antipsychotics']\n",
    "calcium_channel_blockers = datasets['calcium_channel_blockers']\n",
    "oral_hypoglycemics = datasets['oral_hypoglycemics']\n",
    "pancreatic_surgery = datasets['pancreatic_surgery']\n",
    "\n",
    "# combine all datasets\n",
    "all_datasets = pl.DataFrame(\n",
    ")\n",
    "\n",
    "for subject, dataset in datasets.items():\n",
    "    all_datasets = all_datasets.vstack(dataset.with_columns(pl.lit(subject).alias('dataset')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Abstracts and Articles\n",
    "Remove non-English abstracts and abstracts with more than 489 words.\n",
    "\n",
    "Afterwards, remove articles without an abstract, as those do not contain sufficient information to decide on inclusion or exclusion in title/abstract-screening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with duplicates known from manual inspection of the data\n",
    "known_duplicates = {\n",
    "    'animal_depression': ['doi', 'openalex_id', 'pubmed_id'],\n",
    "    'pancreatic_surgery': ['doi', 'pubmed_id', 'webofscience_id']\n",
    "}\n",
    "\n",
    "for subject, dataset in datasets.items():\n",
    "    if subject in known_duplicates.keys():\n",
    "        for column in known_duplicates[subject]:\n",
    "\n",
    "            # save nulls as they are lost to .unique()\n",
    "            null = datasets[subject].filter(\n",
    "                pl.col(column).is_null()\n",
    "            )\n",
    "\n",
    "            # drop duplicate values - also drops null values\n",
    "            unique = datasets[subject].filter(\n",
    "                pl.col(column).is_unique()\n",
    "            )\n",
    "\n",
    "            # combine the unique and null values\n",
    "            filtered = unique.vstack(null)\n",
    "\n",
    "            # override the dataset with the filtered dataset\n",
    "            datasets[subject] = filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-English Abstracts\n",
    "Hard-code the manually determined indices of the English and non-English abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actually_english = {\n",
    "    'adhd': [521],\n",
    "    'animal_depression': [16, 743, 1528],\n",
    "    'pancreatic_surgery': [15500, 24419, 24904, 24951, 29967]\n",
    "}\n",
    "\n",
    "not_english = {\n",
    "    'adhd': [667,759,785,801],\n",
    "    'animal_depression': [266, 421, 675, 848, 968, 1162, 1250, 1521, 1644, 1919, 1947],\n",
    "    'pancreatic_surgery': [200, 17096, 17944]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the language code for the actual English abstracts with 'en'. Then, remove the abstracts of articles with a language code other than 'en'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, dataset in datasets.items():\n",
    "    if subject in actually_english.keys():\n",
    "        dataset =  dataset.with_columns(\n",
    "            pl.when(\n",
    "                pl.col('index').is_in(actually_english[subject]),\n",
    "            ).then(\n",
    "                pl.lit('en')\n",
    "            ).otherwise(\n",
    "                pl.col('language_abstract')).alias('language_abstract'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dataset = dataset.with_columns(\n",
    "            pl.when(\n",
    "                pl.col('index').is_in(not_english[subject]),\n",
    "            ).then(pl.lit(None))\n",
    "            .otherwise(pl.col('abstract'))\n",
    "            .alias('abstract')\n",
    "        )\n",
    "\n",
    "        datasets[subject] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that non-English abstracts were indeed removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['adhd'].vstack(\n",
    "    datasets['animal_depression']\n",
    ").vstack(\n",
    "    datasets['pancreatic_surgery']\n",
    ").filter(\n",
    "    pl.col('language_abstract') != 'en'\n",
    ").select(['index', 'abstract', 'language_abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Abstracts\n",
    "Remove abstracts that have more words than the limit of 489 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_LIMIT = 489\n",
    "\n",
    "for subject, dataset in datasets.items():\n",
    "\n",
    "    # list of indices of abstracts that are above the word limit\n",
    "    idx_long_abstracts = dataset.filter(\n",
    "        pl.col('abstract_word_count') > WORD_LIMIT\n",
    "    ).select('index').to_series().to_list()\n",
    "\n",
    "    # remove abstracts that are above the word limit\n",
    "    dataset = dataset.with_columns(\n",
    "            pl.when(\n",
    "                pl.col('index').is_in(idx_long_abstracts),\n",
    "            ).then(pl.lit(None))\n",
    "            .otherwise(pl.col('abstract'))\n",
    "            .alias('abstract')\n",
    "        )\n",
    "    \n",
    "    datasets[subject] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Abstracts\n",
    "Remove articles without an abstract as they do not contain sufficient information to decide on inclusion or exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to document, how many abstracts are removed\n",
    "documentation = pl.DataFrame(\n",
    "    {\n",
    "        'dataset': datasets.keys(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# lengths of the datasets before removing articles with empty abstracts\n",
    "lengths_before = [len(dataset) for dataset in datasets.values()]\n",
    "\n",
    "# add the initial lengths of the datasets\n",
    "documentation = documentation.with_columns(\n",
    "    pl.Series(\"length_before\", lengths_before)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove articles with empty abstracts\n",
    "for subject, dataset in datasets.items():\n",
    "    datasets[subject] = dataset.filter(\n",
    "        pl.col('abstract').is_not_null()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate, how many articles were removed due to missing abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths of the datasets after removing articles with empty abstracts\n",
    "lengths_after = [len(dataset) for dataset in datasets.values()]\n",
    "\n",
    "# how many articles were removed, absolutely and relative to the total\n",
    "documentation.with_columns(\n",
    "    pl.Series(\"length_after\", lengths_after)\n",
    ").with_columns(\n",
    "    (\n",
    "        pl.col('length_before') - pl.col('length_after')\n",
    "    ).alias('abstracts_removed')\n",
    ").with_columns(\n",
    "    (\n",
    "        pl.col('abstracts_removed') / pl.col('length_before')\n",
    "    ).alias('percentage_removed')\n",
    ").sort(by='percentage_removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal\n",
    "### HTML\n",
    "HTML tags cause noise within the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = datasets['adhd'].to_pandas().iloc[[456, 565]].abstract.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to automatically remove HTML from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text: str)-> str:\n",
    "    \"\"\"Remove html tags from a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing html tags\n",
    "\n",
    "    Returns:\n",
    "    str: a string without html tags\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove HTML from all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import warnings\n",
    "\n",
    "# BeautifulSoup thinks some titles to be similar to filenames \n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "for subject, dataset in datasets.items():\n",
    "    datasets[subject] = dataset.with_columns([\n",
    "        pl.col('title').map_elements(remove_html, return_dtype=pl.String),\n",
    "        pl.col('abstract').map_elements(remove_html, return_dtype=pl.String)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that HTML was indeed removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = datasets['adhd'].to_pandas().iloc[[456, 565]].abstract.values\n",
    "\n",
    "print('With HTML:', end='\\n\\n')\n",
    "[print(abstract[:100], end='\\n') for abstract in before];\n",
    "print('\\n\\nWithout HTML:', end='\\n\\n')\n",
    "[print(abstract[:100], end='\\n') for abstract in after];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characters Only\n",
    "Remove all digits and special characters by regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text: str) -> str:\n",
    "    \"\"\"Remove special characters from a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing special characters\n",
    "\n",
    "    Returns:\n",
    "    str: a string without special characters\n",
    "    \"\"\"\n",
    "    # remove newlines and carriage returns\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    # matches characters only\n",
    "    pattern = r'[^a-zA-Z\\s]+'\n",
    "\n",
    "    # apply the pattern to clean the string\n",
    "    clean_string = re.sub(pattern, '', text)\n",
    "\n",
    "    # ensure that there are no multiple spaces\n",
    "    clean_string = ' '.join(clean_string.split())\n",
    "\n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate that the function works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'This,  i2s A  t3st!\\n\\r4nd  1t  w0rks.'\n",
    "remove_special_characters(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to all titles and abstracts to keep characters only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, dataset in datasets.items():\n",
    "    datasets[subject] = dataset.with_columns([\n",
    "        pl.col('title').map_elements(\n",
    "            remove_special_characters,\n",
    "            return_dtype=pl.String\n",
    "        ),\n",
    "        pl.col('abstract').map_elements(\n",
    "            remove_special_characters,\n",
    "            return_dtype=pl.String\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "Export the preprocessed data for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, dataset in datasets.items():\n",
    "    dataset.write_csv(f'{output_directory}/{subject}_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
