{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed\n",
    "\n",
    "Each dataset contains entries with missing text data. \n",
    "\n",
    "Since an entry without any text is unusable for classification, the goal is to retrieve as many texts as possible.\n",
    "\n",
    "This notebook therefore aims to retrieve texts for each article without text and an associated PubMed id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # file system operations\n",
    "from metapub import PubMedFetcher # pip install metapub\n",
    "from tqdm.notebook import tqdm # progress bars\n",
    "from src import data # helper functions\n",
    "import pandas as pd, polars as pl # dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubMedFetcher\n",
    "\n",
    "The PubMedFetcher library enables to retrieve text data from PubMed through the NCBI repository.\n",
    "\n",
    "Without an api key, the interface is limited to three queries per second.\n",
    "We therefore set an api key as an environment variable.\n",
    "\n",
    "If you do not have an api key yourself, grab one from here:\n",
    "https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv # read .env files\n",
    "\n",
    "# import the ncbi api key from a local .env file\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "ncbi_api_key = os.getenv('NCBI')\n",
    "\n",
    "#set the api key as an environment variable to increase the rate limit\n",
    "os.environ['NCBI_API_KEY'] = ncbi_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_openalex = '../../../../data/datasets/02_openalex'\n",
    "pl_datasets = data.dict_from_directory(data_directory_openalex, separator=',', type='polars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Literature IDs\n",
    "## SYNERGY\n",
    "The SYNERGY datasets refer to ids for PubMed as 'literature_id'. \n",
    "\n",
    "Rename to 'pubmed_id' for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, dataset in pl_datasets.items():\n",
    "    if subject != 'pancreatic_surgery':\n",
    "        pl_datasets[subject] = dataset.rename({'literature_id': 'pubmed_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVIglance\n",
    "The EVIglance dataset contains ids from pubmed, cochrane central, web of science and others. \n",
    "Detect the type of the first three ids and assign them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_datasets['pancreatic_surgery'] = pl_datasets['pancreatic_surgery'].with_columns(\n",
    "    pl.when(pl.col('literature_id').str.starts_with('CN-'))\n",
    "    .then(pl.col('literature_id').alias('central_id')),\n",
    "    pl.when(pl.col('literature_id').str.starts_with('WOS:'))\n",
    "    .then(pl.col('literature_id').alias('webofscience_id')),\n",
    "    pl.when(pl.col('literature_id').str.contains(\"^(\\\\d)*$\"))\n",
    "    .then(pl.col('literature_id').alias('pubmed_id')),\n",
    ").select(pl.all().exclude('literature_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting\n",
    "The PubMed ID is wrongfully formatted as Float64 in the animal depression dataset and as String in the pancreatic surgery dataset. \n",
    "Cast both to Integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_datasets['animal_depression'] = pl_datasets['animal_depression'].cast({pl.Float64: pl.Int64})\n",
    "pl_datasets['pancreatic_surgery'] = pl_datasets['pancreatic_surgery'].cast({'pubmed_id': pl.Int64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Define a uniform schema that all datasets will use from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pl.Schema({\n",
    "    \"include\": pl.Boolean,\n",
    "    \"title\": pl.String,\n",
    "    \"abstract\": pl.String,\n",
    "    \"first_author\": pl.String,#\n",
    "    \"year\": pl.Int16,#\n",
    "    \"journal\": pl.String,#\n",
    "    \"doi\": pl.String,\n",
    "    \"pubmed_id\": pl.Int64,\n",
    "    \"authors\": pl.String,#\n",
    "    \"pubmed_type\": pl.String,#\n",
    "    \"publication_types\": pl.String,#\n",
    "    \"mesh\": pl.String,#\n",
    "    \"webofscience_id\": pl.String,#\n",
    "    \"central_id\": pl.String,#\n",
    "    \"openalex_id\": pl.String,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "Assign the schema to all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, dataset in pl_datasets.items():\n",
    "    df = pl.DataFrame(schema=schema)\n",
    "    pl_datasets[subject] = pl.concat(items=[pl.DataFrame(schema=schema), dataset], how='diagonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Function\n",
    "\n",
    "Define a function which retrieves missing data from PubMed in the following way:\n",
    "\n",
    "- Download article data for each article that contains a PubMed-ID\n",
    "- Fill each empty field in the schema with the downloaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_from_pubmed(dataframe: pd.DataFrame, subject: str) -> pd.DataFrame:\n",
    "\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    for INDEX, row in tqdm(\n",
    "        df.iterrows(), \n",
    "        total=df.shape[0], \n",
    "        desc=subject, \n",
    "        leave=True\n",
    "    ):\n",
    "\n",
    "        if not pd.isnull(row['pubmed_id']):\n",
    "            try:\n",
    "                metadata = PubMedFetcher().article_by_pmid(row['pubmed_id'])\n",
    "\n",
    "                # fill columns if they are missing\n",
    "                df.at[INDEX, 'title'] = metadata.title if pd.isnull(\n",
    "                    row['title']) else row['title']\n",
    "                df.at[INDEX, 'abstract'] = metadata.abstract if pd.isnull(\n",
    "                    row['abstract']) else row['abstract']\n",
    "                df.at[INDEX, 'first_author'] = metadata.author1_last_fm if pd.isnull(\n",
    "                    row['first_author']) else row['first_author']\n",
    "                df.at[INDEX, 'year'] = metadata.year if pd.isnull(\n",
    "                    row['year']) else row['year']\n",
    "                df.at[INDEX, 'journal'] = metadata.journal if pd.isnull(\n",
    "                    row['journal']) else row['journal']\n",
    "                df.at[INDEX, 'doi'] = metadata.doi if pd.isnull(\n",
    "                    row['doi']) else row['doi']\n",
    "                df.at[INDEX, 'pubmed_id'] = metadata.pmid if pd.isnull(\n",
    "                    row['pubmed_id']) else row['pubmed_id']\n",
    "                df.at[INDEX, 'authors'] = metadata.authors_str if pd.isnull(\n",
    "                    row['authors']) else row['authors']\n",
    "                df.at[INDEX, 'pubmed_type'] = metadata.pubmed_type if pd.isnull(\n",
    "                    row['pubmed_type']) else row['pubmed_type']\n",
    "                df.at[INDEX, 'publication_types'] = '; '.join([f'{key}: {value}' for key, value in metadata.publication_types.items(\n",
    "                )]) if pd.isnull(row['publication_types']) else row['publication_types']\n",
    "                df.at[INDEX, 'mesh'] = '; '.join([f'{key}: {value['descriptor_name']}' for key, value in metadata.mesh.items(\n",
    "                )]) if pd.isnull(row['mesh']) else row['mesh']\n",
    "                df.at[INDEX, 'webofscience_id'] = row['webofscience_id']\n",
    "                df.at[INDEX, 'central_id'] = row['central_id']\n",
    "                df.at[INDEX, 'openalex_id'] = row['openalex_id']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Export\n",
    "Apply the aforementioned function to retrieve missing data.\n",
    "\n",
    "Save the extended dataframes to .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_uniform =  '../../../../data/datasets/03_pubmed'\n",
    "\n",
    "for subject, dataset in pl_datasets.items():\n",
    "    # download metadata from pubmed eutils\n",
    "    downloaded_df = fill_missing_from_pubmed(dataset.to_pandas(), subject)\n",
    "\n",
    "    # transform the dataframe to polars\n",
    "    polars_df = pl.DataFrame(data=downloaded_df, schema=schema)\n",
    "    \n",
    "    # save directly to csv \n",
    "    polars_df.write_csv(\n",
    "        f'{data_directory_uniform}/{subject}_pubmed.csv', \n",
    "        index=False\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
