{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Recompile the specifically preprocessed datasets in another subfolder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning - specific Preprocessing\n",
    "This notebook contains code to preprocess the articles in a way specifically tailored to supervised machine learning.\n",
    "Since SML models in this project use term frequency - inverse document frequency, we stem the words to their lexical root form and get rid of unwanted noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data\n",
    "import polars as pl\n",
    "import nltk\n",
    "\n",
    "data_directory = '../../../data/datasets/03_pubmed'\n",
    "datasets = data.dict_from_directory(data_directory, type='polars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text: str)-> str:\n",
    "    \"\"\"Remove html tags from a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing html tags\n",
    "\n",
    "    Returns:\n",
    "    str: a string without html tags\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text:str, digits_also:bool=True,) -> str:\n",
    "    \"\"\"Remove special characters from a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing special characters\n",
    "\n",
    "    Returns:\n",
    "    str: a string without special characters\n",
    "    \"\"\"\n",
    "    expression = '[^ A-Za-z]+' if digits_also else '[^ A-Za-z0-9]+'\n",
    "\n",
    "    specials_removed = re.sub(expression, ' ', text)\n",
    "\n",
    "    return ' '.join(word.strip() for word in specials_removed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test Next word'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'This! is. a, test?Next word'\n",
    "\n",
    "remove_special_characters(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "def correct_spelling(text:str) -> str:\n",
    "    \"\"\"Correct the spelling of a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing misspelled words\n",
    "\n",
    "    Returns:\n",
    "    str: a string with corrected spelling\n",
    "    \"\"\"\n",
    "    spell = Speller(lang='en')\n",
    "    return spell(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemm_text(text:str) -> str:\n",
    "    \"\"\"Stem a text\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string to stem\n",
    "\n",
    "    Returns:\n",
    "    str: a stemmed string\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatize_text(input: str) -> str:\n",
    "    \"\"\"Lemmatize a text\n",
    "\n",
    "    Args:\n",
    "        input: str: a string to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: a lemmatized string\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(input)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    output = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        if wordnet_pos:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        else:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        output.append(lemmatized_word)\n",
    "\n",
    "    return ' '.join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatize_wo_pos(input: str) -> str:\n",
    "    \"\"\"Lemmatize a text\n",
    "\n",
    "    Args:\n",
    "        input: str: a string to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: a lemmatized string\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(input)\n",
    "\n",
    "    output = []\n",
    "    for word in tokens:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        output.append(lemmatized_word)\n",
    "\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemm_text(text:str) -> str:\n",
    "    \"\"\"Stem a text\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string to stem\n",
    "\n",
    "    Returns:\n",
    "    str: a stemmed string\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "def digits_to_words(match):\n",
    "  \"\"\"\n",
    "  Convert string digits to the English words. The function distinguishes between\n",
    "  cardinal and ordinal.\n",
    "  E.g. \"2\" becomes \"two\", while \"2nd\" becomes \"second\"\n",
    "\n",
    "  Input: str\n",
    "  Output: str\n",
    "  \"\"\"\n",
    "  suffixes = ['st', 'nd', 'rd', 'th']\n",
    "  # Making sure it's lower cased so not to rely on previous possible actions:\n",
    "  string = match[0].lower()\n",
    "  if string[-2:] in suffixes:\n",
    "    type='ordinal'\n",
    "    string = string[:-2]\n",
    "  else:\n",
    "    type='cardinal'\n",
    "\n",
    "  return num2words(string, to=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords.\n",
    "\n",
    "    Input: str\n",
    "    Output: str\n",
    "    \"\"\"\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\root\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def preprocess_dataframe(dataframe: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a dataframe by removing html tags, special characters, spelling mistakes, lemmatizing (using part-of-speech-tags), and removing stopwords.\n",
    "\n",
    "    Args:\n",
    "    dataframe: pl.DataFrame: a dataframe with columns 'title' and 'abstract' containing text to preprocess\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: a dataframe with  preprocessed text\n",
    "    \"\"\"\n",
    "    return dataframe.with_columns(\n",
    "        pl.col(\"title\")\n",
    "        .str.to_lowercase()  # Vectorized operation\n",
    "        .map_elements(remove_html, return_dtype=pl.String)  # Custom function for complex logic\n",
    "        .map_elements(lemmatize_text, return_dtype=pl.String)\n",
    "        .map_elements(remove_special_characters, return_dtype=pl.String)\n",
    "        .map_elements(lambda x: re.sub(r'\\d+(st)?(nd)?(rd)?(th)?', digits_to_words, x), return_dtype=pl.String)\n",
    "        .map_elements(remove_stop_words, return_dtype=pl.String),\n",
    "        pl.col('abstract')\n",
    "        .str.to_lowercase()  # Vectorized operation\n",
    "        .map_elements(remove_html, return_dtype=pl.String)  # Custom function for complex logic\n",
    "        .map_elements(lemmatize_text, return_dtype=pl.String)\n",
    "        .map_elements(remove_special_characters, return_dtype=pl.String)\n",
    "        .map_elements(lambda x: re.sub(r'\\d+(st)?(nd)?(rd)?(th)?', digits_to_words, x), return_dtype=pl.String)\n",
    "        .map_elements(remove_stop_words, return_dtype=pl.String),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:49<00:00, 58.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data_directory_preprocessing = '../../../data/datasets/04_preprocessed/supervised'\n",
    "\n",
    "print('Preprocessing datasets...')\n",
    "for subject, dataset in tqdm(datasets.items(), total=len(datasets)):\n",
    "    datasets[subject] = preprocess_dataframe(dataset)\n",
    "\n",
    "    datasets[subject].write_csv(f'{data_directory_preprocessing}/{subject}_preprocessed_sml.csv')\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
