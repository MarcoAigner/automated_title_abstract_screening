{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data\n",
    "import polars as pl\n",
    "import nltk\n",
    "\n",
    "data_directory = '../../../../data/05_vocabs'\n",
    "datasets = data.dict_from_directory(data_directory, type='polars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text: str)-> str:\n",
    "    \"\"\"Remove html tags from a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing html tags\n",
    "\n",
    "    Returns:\n",
    "    str: a string without html tags\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text:str, digits_also:bool=True,) -> str:\n",
    "    \"\"\"Remove special characters from a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing special characters\n",
    "\n",
    "    Returns:\n",
    "    str: a string without special characters\n",
    "    \"\"\"\n",
    "    expression = '[^ A-Za-z]+' if digits_also else '[^ A-Za-z0-9]+'\n",
    "\n",
    "    specials_removed = re.sub(expression, ' ', text)\n",
    "\n",
    "    return ' '.join(word.strip() for word in specials_removed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test Next word'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'This! is. a, test?Next word'\n",
    "\n",
    "remove_special_characters(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "def correct_spelling(text:str) -> str:\n",
    "    \"\"\"Correct the spelling of a string\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string containing misspelled words\n",
    "\n",
    "    Returns:\n",
    "    str: a string with corrected spelling\n",
    "    \"\"\"\n",
    "    spell = Speller(lang='en')\n",
    "    return spell(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemm_text(text:str) -> str:\n",
    "    \"\"\"Stem a text\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string to stem\n",
    "\n",
    "    Returns:\n",
    "    str: a stemmed string\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatize_text(input: str) -> str:\n",
    "    \"\"\"Lemmatize a text\n",
    "\n",
    "    Args:\n",
    "        input: str: a string to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: a lemmatized string\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(input)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    output = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        if wordnet_pos:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        else:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        output.append(lemmatized_word)\n",
    "\n",
    "    return ' '.join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatize_wo_pos(input: str) -> str:\n",
    "    \"\"\"Lemmatize a text\n",
    "\n",
    "    Args:\n",
    "        input: str: a string to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: a lemmatized string\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(input)\n",
    "\n",
    "    output = []\n",
    "    for word in tokens:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        output.append(lemmatized_word)\n",
    "\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemm_text(text:str) -> str:\n",
    "    \"\"\"Stem a text\n",
    "    \n",
    "    Args:\n",
    "    text: str: a string to stem\n",
    "\n",
    "    Returns:\n",
    "    str: a stemmed string\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "def digits_to_words(match):\n",
    "  \"\"\"\n",
    "  Convert string digits to the English words. The function distinguishes between\n",
    "  cardinal and ordinal.\n",
    "  E.g. \"2\" becomes \"two\", while \"2nd\" becomes \"second\"\n",
    "\n",
    "  Input: str\n",
    "  Output: str\n",
    "  \"\"\"\n",
    "  suffixes = ['st', 'nd', 'rd', 'th']\n",
    "  # Making sure it's lower cased so not to rely on previous possible actions:\n",
    "  string = match[0].lower()\n",
    "  if string[-2:] in suffixes:\n",
    "    type='ordinal'\n",
    "    string = string[:-2]\n",
    "  else:\n",
    "    type='cardinal'\n",
    "\n",
    "  return num2words(string, to=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords.\n",
    "\n",
    "    Input: str\n",
    "    Output: str\n",
    "    \"\"\"\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def preprocess_dataframe(dataframe: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a dataframe by removing html tags, special characters, spelling mistakes, lemmatizing (using part-of-speech-tags), and removing stopwords.\n",
    "\n",
    "    Args:\n",
    "    dataframe: pl.DataFrame: a dataframe with columns 'title' and 'abstract' containing text to preprocess\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: a dataframe with  preprocessed text\n",
    "    \"\"\"\n",
    "    return dataframe.with_columns(\n",
    "        pl.col(\"title\")\n",
    "        .str.to_lowercase()  # Vectorized operation\n",
    "        .map_elements(remove_html, return_dtype=pl.String)  # Custom function for complex logic\n",
    "        .map_elements(lemmatize_text, return_dtype=pl.String)\n",
    "        .map_elements(remove_special_characters, return_dtype=pl.String)\n",
    "        .map_elements(lambda x: re.sub(r'\\d+(st)?(nd)?(rd)?(th)?', digits_to_words, x), return_dtype=pl.String)\n",
    "        .map_elements(remove_stop_words, return_dtype=pl.String),\n",
    "        pl.col('abstract')\n",
    "        .str.to_lowercase()  # Vectorized operation\n",
    "        .map_elements(remove_html, return_dtype=pl.String)  # Custom function for complex logic\n",
    "        .map_elements(lemmatize_text, return_dtype=pl.String)\n",
    "        .map_elements(remove_special_characters, return_dtype=pl.String)\n",
    "        .map_elements(lambda x: re.sub(r'\\d+(st)?(nd)?(rd)?(th)?', digits_to_words, x), return_dtype=pl.String)\n",
    "        .map_elements(remove_stop_words, return_dtype=pl.String),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(f'{data_directory}/adhd_vocabs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Expr' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[0;32m      2\u001b[0m     \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_lowercase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      6\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mto_lowercase()\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mfaig\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\polars\\expr\\expr.py:878\u001b[0m, in \u001b[0;36mExpr.pipe\u001b[1;34m(self, function, *args, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    831\u001b[0m     function: Callable[Concatenate[Expr, P], T],\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    834\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124;03m    Offers a structured way to apply a sequence of user-defined functions (UDFs).\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m \n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mremove_html\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_html\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Remove html tags from a string\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    str: a string without html tags\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_text()\n",
      "File \u001b[1;32mc:\\Users\\mfaig\\miniforge3\\envs\\automated_screening\\Lib\\site-packages\\bs4\\__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    318\u001b[0m ):\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Expr' has no len()"
     ]
    }
   ],
   "source": [
    "df.with_columns(\n",
    "    pl.col(\n",
    "        'title').str.to_lowercase().pipe(\n",
    "            remove_html,\n",
    "        ),\n",
    "    pl.col('abstract').str.to_lowercase()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [07:54<00:00, 79.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# data_directory_preprocessing = '../../../../data/06_preprocessed'\n",
    "\n",
    "# print('Preprocessing datasets...')\n",
    "# for subject, dataset in tqdm(datasets.items(), total=len(datasets)):\n",
    "#     datasets[subject] = preprocess_dataframe(dataset)\n",
    "\n",
    "#     datasets[subject].write_csv(f'{data_directory_preprocessing}/{subject}_preprocessed.csv')\n",
    "# print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
