{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Supervised Machine Learning\n",
    "## Preparation\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import polars as pl\n",
    "from src import data\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the data is stored\n",
    "data_directory = '../../../data/datasets/04_preprocessed'\n",
    "\n",
    "# load the data\n",
    "datasets = data.dict_from_directory(data_directory, type='polars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(df):\n",
    "    \"\"\"\n",
    "    Return a train-test split of the data\n",
    "\n",
    "    Args:\n",
    "        df: Polars DataFrame with columns 'title', 'abstract', and 'include'.\n",
    "\n",
    "    Returns:\n",
    "        X_train: list of strings, training data.\n",
    "        X_test: list of strings, test data.\n",
    "        y_train: list of booleans, training labels.\n",
    "        y_test: list of booleans, test labels.\n",
    "    \"\"\"\n",
    "    # combine title and abstract into one column\n",
    "    combined = df.select(\n",
    "        pl.concat_str(\n",
    "            pl.col('title').fill_null(''),\n",
    "            pl.col('abstract').fill_null(''),\n",
    "            separator=' ',\n",
    "        ).alias('text')\n",
    "    )\n",
    "    # the vectorizer does not accept null values\n",
    "    combined = combined.fill_null('')\n",
    "\n",
    "    # features\n",
    "    X = combined.to_series().to_list()\n",
    "\n",
    "    # target\n",
    "    y = df['include'].to_list()\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=0.3, \n",
    "        random_state=42,  # important for reproducibility\n",
    "        stratify=y # important for imbalanced classes\n",
    "    ) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(estimator):\n",
    "    \"\"\"\n",
    "    Return a pipeline ready encapuslating a given estimator.\n",
    "    The pipeline includes Tf-idf vectorization and random undersampling.\n",
    "\n",
    "    Args:\n",
    "        estimator: scikit-learn estimator.\n",
    "    \n",
    "    Returns:\n",
    "        pipeline: imbalanced-learn pipeline.\n",
    "    \"\"\"\n",
    "    return Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('undersampling', RandomUnderSampler(\n",
    "            sampling_strategy='auto',\n",
    "            random_state=42\n",
    "            )\n",
    "        ),\n",
    "        ('estimator', estimator)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "### Defining Estimators\n",
    "Article classes will be predicted by the following four estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of estimators to predict with\n",
    "# estimators use class_weights as available\n",
    "estimators = {\n",
    "    'logistic_regression': LogisticRegression(class_weight='balanced'),\n",
    "    'random_forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'support_vector_machine': SVC(class_weight='balanced'),\n",
    "    'naive_bayes': ComplementNB(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Article Classes\n",
    "Predict classes for each dataset and estimator each. Save the predictions besides the true values of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea32ec7a799d402c83be2ef79a57c197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429974ff58ec4e37bb32d948d3641f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08c8e04c90a46e58047972b9e2e8073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839fd16bacd14472a2f05bdd343a7775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fab95e9b6b94352be06a27eb87786dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c8ba2c1def4487a97e2a0fedff09d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498a01858822484e8acf665eeb0b248e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimators:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for subject, dataset in tqdm(\n",
    "    iterable=datasets.items(),\n",
    "    desc='Datasets',\n",
    "    total=len(datasets),\n",
    "    leave=True\n",
    "):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = create_train_test_sets(dataset)\n",
    "\n",
    "    predictions[subject] = pl.DataFrame(\n",
    "        data=pl.Series(\n",
    "            name='true',\n",
    "            values=y_test\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for name, estimator in tqdm(\n",
    "        iterable=estimators.items(),\n",
    "        desc='Estimators',\n",
    "        total=len(estimators),\n",
    "        leave=False,\n",
    "    ):\n",
    "        \n",
    "        # pipeline\n",
    "        pipeline = create_pipeline(estimator)\n",
    "\n",
    "        # fit\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # store predictions\n",
    "        predictions[subject] = predictions[subject].with_columns(\n",
    "            pl.Series(name=name, values=y_pred)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save the predictions\n",
    "export_path = '../../../data/predictions/supervised_machine_learning'\n",
    "\n",
    "for subject, df in predictions.items():\n",
    "    df.write_csv(f'{export_path}/{subject}_pred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
